{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ca1798-a383-420d-9693-f3a2b2b59335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sentiment Analysis with Amazon Reviews - Classic Compute\n",
    "\n",
    "+ Use the open-sourced Amazon Review data-set to experiment with Sentiment Analysis on customer reviews.  \n",
    "+ Use only Databricks Classic Compute with ML DBR\n",
    "\n",
    "Trial with 15.4 ML DBR (non GPU). Also tested with 15.4 ML GPU to speed up Embeddings generation.\n",
    "\n",
    "**Feature Data**.   \n",
    "The *Features Table* has been pre-generated with the first section of the notebook \"*Amazon Reviews Sentiment*\" (sections \"Load Sample Dataset to {catalog}.{database}.{table}\" and \"Create a Features Table with RowIDs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**SiEBERT**.   \n",
    "SiEBERT is a sophisticated sentiment analysis model built on RoBERTa-large architecture, specifically designed for binary sentiment classification in English text. Fine-tuned on 15 diverse datasets, it demonstrates exceptional generalization capabilities across various text types, from product reviews to social media content.\n",
    "\n",
    "\n",
    "Examples: \n",
    "+ Cardiff NLP Twitter-roBERTa-base: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment   \n",
    "+ Kaggle with RoBERTa transformer: https://www.kaggle.com/code/stefancomanita/sentiment-analysis-with-hugging-face-transformers\n",
    "+ SiEBERT: https://huggingface.co/siebert/sentiment-roberta-large-english \n",
    "\n",
    "SiEBERT is a binary classifier,  Cardif NLP is just good, bad, medium\n",
    "\n",
    "Target approach:\n",
    "- Use SiEBERT (or any transformer model) to extract [CLS] embeddings \n",
    "- These embeddings capture sentiment, semantic structure, and topical signals\n",
    "- Train multiple downstream models\n",
    "- One for star rating prediction (classification or regression).\n",
    "- One for topic classification (multi-class or multi-label).\n",
    "- Possibly one for spam detection, toxicity, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fd2d11-c358-4fa2-be45-92e7e893ade4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PART 1 - GENERAL Environment Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62054656-3f52-4916-9900-f8b824c6b8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parameterize the Catalog and Schema Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43fd71f0-0b56-4e40-8648-a111e7ed88b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"users\")\n",
    "dbutils.widgets.text(\"database\", \"\")\n",
    "dbutils.widgets.text(\"features_table\", \"review_features\")  # this table should be pre-created and populated with data\n",
    "dbutils.widgets.text(\"embeddings_table\", \"review_embeddings\")   # this gets created and populated with this notebook\n",
    "dbutils.widgets.text(\"predictions_table\", \"review_cl_predictions\")  # CLASSIC predictions table created and populated with this notebook - GIVE IT A DIFFERENT NAME to GenAI example notebook  \n",
    "dbutils.widgets.text(\"experiment_name\", \"/Users/ed.bullen@databricks.com/rating_model_experiment\")  # MLFlow model training experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b022aec-dc75-422c-ab53-aa3fad05613f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Table location params set to\", dbutils.widgets.getAll())\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "database = dbutils.widgets.get(\"database\")\n",
    "features_table = dbutils.widgets.get(\"features_table\")  # Amazon reviews with a surrogate ID key\n",
    "embeddings_table = dbutils.widgets.get(\"embeddings_table\")  \n",
    "predictions_table = dbutils.widgets.get(\"predictions_table\")\n",
    "experiment_name = dbutils.widgets.get(\"experiment_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebf4182-ddb1-439d-be21-9d2b3457b23a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Preview the review_features table"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM `${catalog}`.`${database}`.`${features_table}`\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e551ea9d-dbe9-4495-8ffc-b1d812f79128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0055d58d-745a-49d8-ab23-c17aeb5983cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256745d3-f967-4d5c-b65f-0be37dae46a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This should already be in place for DBR 15.4 ML\n",
    "#%pip install -U transformers scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cde0ad3-4c97-46f6-b90f-93deb9315a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Huggingface Model Download** \n",
    "This step downloads the `sentiment-roberta-large-english` model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbe4f09-da9f-4524-aac9-d43e7d343285",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Siebert Embeddings Tokenizer function"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model = AutoModel.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "model.eval()\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Simple non-parallized get_embedding() function for small data-sets only. See UDF function in PART 2 for bulk operations\n",
    "#def get_embedding(text):\n",
    "#    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "#    with torch.no_grad():\n",
    "#        outputs = model(**inputs)\n",
    "#    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cee23b-a10b-4f60-a34c-391d57e0dd13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "alternative embeddings function"
    }
   },
   "outputs": [],
   "source": [
    "# Batch embedding function - process in batches to cut down tokenization overhead and improve throughput\n",
    "#def get_batch_embeddings(texts, batch_size=16):\n",
    "#    all_embeddings = []\n",
    "#    for i in range(0, len(texts), batch_size):\n",
    "#        batch_texts = texts[i:i+batch_size]\n",
    "#        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "#        with torch.no_grad():\n",
    "#            outputs = model(**inputs)\n",
    "#        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "#        all_embeddings.append(batch_embeddings)\n",
    "#    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98bb5e2-9c21-43ca-b427-d98efdd1b6af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the embeddings extract in a UDF Function"
    }
   },
   "outputs": [],
   "source": [
    "# Spark parallel cluster exec - get embeddings\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define the scalar iterator UDF for batch processing\n",
    "@pandas_udf(ArrayType(FloatType()), functionType=PandasUDFType.SCALAR_ITER)\n",
    "def generate_embeddings(reviews_iter):\n",
    "    # Load model/tokenizer once per partition\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "    model = AutoModel.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "    model.eval()\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for reviews in reviews_iter:\n",
    "        embeddings = []\n",
    "        for text in reviews:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                vector = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(vector.astype(float).tolist())\n",
    "        yield pd.Series(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fd3726-ae63-47e0-b2bc-99e1bc0409ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check if GPU is available"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8096c57-d494-4f99-9223-1f9f34480010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple non-parallized get_embedding() function returns same pd.Series(embeddings) format as the UDF\n",
    "#def get_embeddings_series(text):\n",
    "#    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "#    with torch.no_grad():\n",
    "#        outputs = model(**inputs)\n",
    "#    vector = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "#    return pd.Series([vector.astype(float).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f8b084-e2e3-4c02-80b2-e5fafbacb227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PART 2 - Embeddings Setup\n",
    "\n",
    "This takes a long time to do without a GPU.  If the model has already been trained, skip forward to \"PART 4 - Use the model to predict\"\n",
    "\n",
    "## Load SiEBERT and Extract Embeddings\n",
    "\n",
    "Only take the first 18859 features, hold the remaining 1000 rows back for a final test of the model that is trained on the selected feature-set (i.e. keep a \"holdout data set\" for later).\n",
    "\n",
    "Running the process to extract embeddings takes a long time without a GPU.  However this can be parallelised on a Spark cluster (with regular CPUs).  Despite only having a small data-set, parition it up into (say) 64 partitions so that the embeddings extract can be split in parallel across cluster nodes and CPU cores.  In this example 64 partitions were created to distribute accross a 4 node cluster with 4 cores on each node. This reduced processing time from nearly 2 hours to ~ 20 minutes.  Add more nodes for faster processing.\n",
    "\n",
    "A Pandas UDF called `generated_embeddings()`is defined and then used to wrap the embeddings generation into a Spark worker-node execution distributed in parallel around the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36bb86d4-4b57-4dcf-8eec-f7a9467bc8ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load data into Spark Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# load data into a spark DF for cluster processing and force it to be partitioned (despite it being small)\n",
    "spark_df = spark.sql(f\"SELECT * FROM {catalog}.{database}.{features_table} WHERE id <= 18859\").repartition(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ee8971-c8ca-409f-8210-80b8946cc87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extract Embeddings with SiEBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2da9531-00cb-491f-93eb-b422e6004b6b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Spark DF Partitions Number"
    }
   },
   "outputs": [],
   "source": [
    "# check the number of partitions to see that the data-frame is distributed in the cluster and suitable for multiple worker processes \n",
    "print(spark_df.rdd.getNumPartitions())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b311a93-00d7-4b95-bb2f-5c9573e83ecd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate embeddings"
    }
   },
   "outputs": [],
   "source": [
    "# Add the embeddings as a new column to spark_df, using the embedding UDF\n",
    "spark_df = spark_df.withColumn(\"embeddings\", generate_embeddings(spark_df[\"review\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6ee055-dac5-4b28-81e7-957e1d908945",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "display the spark DF to kick Spark lazy execution into action"
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6133e047-4789-4c4c-8aa5-415dad060662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecac9382-36f5-41f7-85ed-6299474d7c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PART 3 - Use the Embeddings and Train a Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103abc02-e40f-4995-8ed7-477b6790c982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train a Classifier - Rating 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d9dd55-1bc2-47d2-af95-8e463200f365",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Model Registry details"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#import mlflow.sklearn\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "# Set the MLflow registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_registry_name = f\"{catalog}.{database}.rating_model\"\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bf7d1c-7762-464b-9e2e-73de016942aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Copy to driver - 15 mins"
    }
   },
   "outputs": [],
   "source": [
    "# spark_df copied to pandas_df (now running single threaded in the driver node)\n",
    "#pandas_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82df5923-001a-4a43-b267-51ddcea58c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Taining with UDF VERSION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16006e84-1811-44c9-b175-59da81cde6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Define the UDF\n",
    "vector_udf = F.udf(lambda arr: Vectors.dense(arr), ArrayType(DoubleType()))\n",
    "\n",
    "# Apply the UDF\n",
    "df_vec = spark_df.withColumn(\"features\", vector_udf(\"embeddings\"))\n",
    "\n",
    "gbt   = GBTRegressor(labelCol=\"rating\", featuresCol=\"features\",\n",
    "                     maxIter=100, stepSize=0.1, maxDepth=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a745bdb-f120-4455-b3c6-1b4d3c522e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Convert array<double> to Vector\n",
    "array_to_vector_udf = udf(lambda array: Vectors.dense(array), VectorUDT())\n",
    "df_vec = df_vec.withColumn('features_vec', array_to_vector_udf(df_vec['features']))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['features_vec'], outputCol='features_vec_assembled')\n",
    "df_vec = assembler.transform(df_vec).drop('features').drop('features_vec').withColumnRenamed('features_vec_assembled', 'features')\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "import mlflow.spark\n",
    "with mlflow.start_run():\n",
    "    mlflow.spark.autolog()\n",
    "    model = gbt.fit(df_vec)       \n",
    "experiment_metadata = dict(mlflow.get_experiment_by_name(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8ca8fa-240d-4b2e-84ac-eed585fa0f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training with UDF and VECTOR ASSEMBER VERSION 2\n",
    "\n",
    "\n",
    "Notes on training and logging:\n",
    "https://docs.databricks.com/aws/en/machine-learning/train-model/xgboost-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec15288-5755-44cb-832a-f72f2535f7d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check PYSPARK_PIN_THREAD (this has to be set in the cluster configuation and meeds to be FALSE for autologging to work) \n",
    "#\n",
    "# Using Databricks UI: \n",
    "# Go to Edit cluster settings.\n",
    "# Scroll to Advanced options and open the Spark tab.\n",
    "# In the **Environment variables** field, enter (or correct): PYSPARK_PIN_THREAD=false\n",
    "\n",
    "import os\n",
    "print(\"PYSPARK_PIN_THREAD =\", os.environ.get(\"PYSPARK_PIN_THREAD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6807df30-166f-4d22-921e-eb802a9c0921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "import mlflow, mlflow.spark\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# 1. DataFrame with a native vector column (no UDF)\n",
    "df = (\n",
    "    spark_df\n",
    "      .select(\"rating\", array_to_vector(\"embeddings\").alias(\"features\"))\n",
    "      .cache()\n",
    ")\n",
    "df.count()   # materialise cache\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "        labelCol=\"rating\",\n",
    "        featuresCol=\"features\",\n",
    "        maxIter=80,\n",
    "        stepSize=0.1,\n",
    "        maxDepth=4,\n",
    "        seed=42\n",
    "     )\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.spark.autolog()\n",
    "    #mlflow.pyspark.ml.autolog()\n",
    "    model = gbt.fit(df)\n",
    "    experiment_metadata = dict(mlflow.get_experiment_by_name(experiment_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a58d281-419f-44df-877c-71a9484a37c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the best model - refer to a specific experiment\n",
    "import json\n",
    "experiment_metadata=dict(mlflow.get_experiment_by_name(experiment_name))\n",
    "print(json.dumps(experiment_metadata,indent=4))\n",
    "\n",
    "# find the best model - metadata unique ID\n",
    "experiment_id=experiment_metadata['experiment_id']\n",
    "\n",
    "# find the best model - search runs in the experiment by various metics\n",
    "mlflow_model = mlflow.search_runs([experiment_id]\n",
    "                                #, filter_string='tags.project = \"my_tag\"'\n",
    "                                #, order_by = ['metrics.f1_score DESC']).iloc[0] # best F1\n",
    "                                , order_by = ['attributes.start_time DESC']).iloc[0] # just get the latest run\n",
    "\n",
    "# show the model details\n",
    "mlflow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d784eb-04d7-4a93-bfbe-2c06ab9ead19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "registered_model = mlflow.register_model(\n",
    "  f\"runs:/{mlflow_model.run_id}/model\", model_registry_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5345283a-a4cf-41af-92aa-9641f0e3ad57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RDD VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1410dfc-8968-4ec2-be27-52738f54488b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f4c219-ea77-4c94-a464-f4c7641a8d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "#from pyspark.ml.classification import BoostingStrategy\n",
    "\n",
    "# Convert the DataFrame to an RDD of LabeledPoint\n",
    "data = spark_df.rdd.map(lambda row: LabeledPoint(row['rating'], Vectors.dense(row['embeddings'])))\n",
    "print(\"Training Partitions:\", data.getNumPartitions())\n",
    "\n",
    "# Set the boosting strategy\n",
    "#boosting_strategy = BoostingStrategy.defaultParams('classification')\n",
    "#boosting_strategy.numIterations = 10  # Number of iterations\n",
    "\n",
    "# Train the Gradient Boosted Trees model\n",
    "#rating_model = GradientBoostedTrees.train(data, boosting_strategy)\n",
    "rating_model = GradientBoostedTrees.trainClassifier(data, {}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f3d0418-fcbd-4d0d-8f96-b5f601121640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rating_model = GradientBoostedTrees.trainClassifier(spark_df, \"rating\", \"embeddings\", {}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebec631-fe76-4f12-9624-4af22e72cb55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train Model - 1 hour"
    }
   },
   "outputs": [],
   "source": [
    "# Takes up to 1 hour to run training against 18859 records\n",
    "#X = pandas_df['embeddings'].apply(lambda x: np.array(x)).tolist()\n",
    "#y = pandas_df['rating']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "# Log the model\n",
    "#with mlflow.start_run():\n",
    "#    mlflow.sklearn.autolog()  # use auto-log to ensure signature gets logged\n",
    "#    #mlflow.sklearn.log_model(rating_model, \"rating_model\")\n",
    "#    rating_model = GradientBoostingClassifier()\n",
    "#    rating_model.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Rating prediction results:\")\n",
    "#print(classification_report(y_test, rating_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc5bd7d5-6ff3-4efc-9873-2f43b3c4a35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get the MLflow model ID and register it in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25775a8b-f8a6-4c42-877a-ffe995fd211c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the best model - refer to a specific experiment\n",
    "import json\n",
    "experiment_metadata=dict(mlflow.get_experiment_by_name(experiment_name))\n",
    "print(json.dumps(experiment_metadata,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03724d16-e88b-4dcb-af32-db631e07a34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the best model - metadata unique ID\n",
    "experiment_id=experiment_metadata['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e89606-c80e-476b-b6f7-0bfb0ed2d1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the best model - search runs in the experiment by various metics\n",
    "mlflow_model = mlflow.search_runs([experiment_id]\n",
    "                                #, filter_string='tags.project = \"my_tag\"'\n",
    "                                #, order_by = ['metrics.f1_score DESC']).iloc[0] # best F1\n",
    "                                , order_by = ['attributes.start_time DESC']).iloc[0] # just get the latest run\n",
    "\n",
    "# show the model details\n",
    "mlflow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f695775-3173-48d2-84ca-498569b114e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "registered_model = mlflow.register_model(\n",
    "  f\"runs:/{mlflow_model.run_id}/model\", model_registry_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35233088-9109-4989-9069-3744ce574c2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "give the model to be used an alias \"live\" so that it is picked up in this notebook."
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set an alias for a specific model version\n",
    "registered_model_name = registered_model.name\n",
    "alias_name = \"live\"\n",
    "\n",
    "client.set_registered_model_alias(registered_model_name, alias_name, registered_model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae6a1bf3-98be-4d9b-91d1-b68adb6d99af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PART 4 - Use the Model to predict Rating\n",
    "\n",
    "uses an alias \"live\" to determine which model version is picked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed5565b-8235-496d-a437-7f659cdf3503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the model to load\n",
    "\n",
    "model_version_uri = f\"models:/{catalog}.{database}.rating_model@live\"\n",
    "\n",
    "# load the model\n",
    "model = mlflow.sklearn.load_model(model_uri=model_version_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d98f5c-239c-48c9-8044-3e37de78c4cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a PySpark UDF and use it for batch inference"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "pyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_version_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e65688d-7c50-4a90-9266-6041d74ee167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load data into a spark DF for cluster processing - single node\n",
    "spark_df = spark.sql(f\"SELECT * FROM {catalog}.{database}.{features_table} WHERE id > 18859\").repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c06337-d86a-4e1f-a0e1-4dce2fe45a36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1100 embeddings, single node, single partition"
    }
   },
   "outputs": [],
   "source": [
    "# Add the embeddings as a new column to spark_df, using the embedding UDF\n",
    "spark_df = spark_df.withColumn(\"embeddings\", generate_embeddings(spark_df[\"review\"]))\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c99ce1-8baf-4d2a-8ca9-373b101514c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert to Pandas DF, predict against X, use y to check accuracy\n",
    "pandas_df = spark_df.toPandas()\n",
    "X = pandas_df['embeddings'].apply(lambda x: np.array(x)).tolist()\n",
    "y = pandas_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0808302-72ec-4f09-8c4f-3eb0013ce034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332645a5-90cc-4828-946c-85b86cce4529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "review_cl_predictions = pd.DataFrame({\n",
    "    'id': pandas_df['id'],\n",
    "    'predictions': predictions,\n",
    "    'rating': pandas_df['rating'],\n",
    "    'review': pandas_df['review']\n",
    "})\n",
    "display(review_cl_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f672fa-9a7f-4046-9c8b-e55b3c9461fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "review_cl_predictions_spark = spark.createDataFrame(review_cl_predictions)\n",
    "review_cl_predictions_spark.write.format(\"delta\").saveAsTable(f\"{catalog}.{database}.review_cl_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdabf4d-ab3f-49ef-b419-d69de9c49574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) from users.ed_bullen.review_cl_predictions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666c4190-9c73-4aba-a18f-0cdf11ea9933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT \n  p.predictions - f.rating as diff,\n  COUNT(*) as count,\n  ROUND(100*COUNT(*)/1100) as percent\nFROM \n  users.ed_bullen.review_cl_predictions p \nJOIN \n  users.ed_bullen.review_features f \nON \n  p.id = f.id\nGROUP BY \n  p.predictions - f.rating\nORDER BY diff) SELECT `diff`,SUM(`count`) `column_29285123415` FROM q GROUP BY `diff`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "diff",
             "id": "column_29285123411"
            },
            "y": [
             {
              "column": "percent",
              "id": "column_29285123415",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_29285123415": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "05334c5c-cf44-4be8-b40f-3b61ffc47427",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 29.7421875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "diff",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "diff",
           "type": "column"
          },
          {
           "alias": "column_29285123415",
           "args": [
            {
             "column": "percent",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  p.predictions - f.rating as diff,\n",
    "  COUNT(*) as count,\n",
    "  ROUND(100*COUNT(*)/1100) as percent\n",
    "FROM \n",
    "  users.ed_bullen.review_cl_predictions p \n",
    "JOIN \n",
    "  users.ed_bullen.review_features f \n",
    "ON \n",
    "  p.id = f.id\n",
    "GROUP BY \n",
    "  p.predictions - f.rating\n",
    "ORDER BY diff;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f3b7d6-bb8a-4032-a0f4-4af4c5c2c8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Conclusion\n",
    "\n",
    "Results for XGBoost + HuggingFace SiEBERT with `sentiment-roberta-large-english`:\n",
    "\n",
    "  \n",
    "+ 71% of the predicted ratings have the same rating prediction as was given by the Amazon customer\n",
    "+ 93% are within +/- 1 rating of the Amazon customer review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d21b3c-10fc-469b-99b2-34cdef798ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1113045451747613,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Amazon Reviews Sentiment Classic Compute",
   "widgets": {
    "catalog": {
     "currentValue": "users",
     "nuid": "b9ad2c95-a9b7-46f8-89c1-71d418d3ebd2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "database": {
     "currentValue": "ed_bullen",
     "nuid": "192e1f48-247a-42c7-ac9a-06eedddbd099",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "embeddings_table": {
     "currentValue": "review_embeddings",
     "nuid": "3731ba15-9209-4f49-af78-9b8fd69fc00f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "review_embeddings",
      "label": null,
      "name": "embeddings_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "review_embeddings",
      "label": null,
      "name": "embeddings_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "experiment_name": {
     "currentValue": "/Users/ed.bullen@databricks.com/rating_model_experiment",
     "nuid": "7328ddc9-b42a-4e98-b2c8-8b557b474cfa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Users/ed.bullen@databricks.com/rating_model_experiment",
      "label": null,
      "name": "experiment_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Users/ed.bullen@databricks.com/rating_model_experiment",
      "label": null,
      "name": "experiment_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "features_table": {
     "currentValue": "review_features",
     "nuid": "c3df7929-0995-4a50-bebd-9d927221c2be",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "review_features",
      "label": null,
      "name": "features_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "review_features",
      "label": null,
      "name": "features_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "predictions_table": {
     "currentValue": "review_cl_predictions",
     "nuid": "26f75876-1e10-4f9e-a90e-7a2353e28baf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "review_cl_predictions",
      "label": null,
      "name": "predictions_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "review_cl_predictions",
      "label": null,
      "name": "predictions_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
